BUGFIXES:
    
TO CHECK:
    Check why net with batchnorm does worst than same net without
    -> working but understand implementations better
    Check Softmax derivative
    Check cross entropy

    Check minmax scaling implementation
    Check glorot normal and both he init implementations
    Check Dropout


TO ADD:
    Add batchnorm gamma and beta gradient checking
    Remove Save and load weights
    Convolutional layer
    Batch normalization
    Deconvolutional layer
    Recurrent layer
    LSTM layer
    GAN architecture
    Attention architecture
    L1 Regularization
