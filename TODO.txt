TO CHECK:
    Check minmax scaling implementation
    Check glorot normal and both he init implementations
    Check Dropout


TO ADD:
    ADAM optimizer
    Save and load weights
    Convolutional layer
    Batch normalization
    CrossEntropy implementation that doesn't depend on Sigmoid in last layer
    Softmax layer
    Deconvolutional layer
    Recurrent layer
    LSTM layer
    GAN architecture
    Attention architecture
