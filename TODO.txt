BUGFIXES:

TO CHECK:

    can replace batch size parameter in layer functions with batch_size = self.error.shape[1]
    Check why net with batchnorm does worst than same net without
    -> working but understand implementations better
    Check minmax scaling implementation
    Check glorot normal and both he init implementations
    Check Dropout


TO ADD:
    Convolutional layer
    Pooling layers 
    Remove Save and load weights
    Add link from layer to next and previous layer, so that we can calculate 
    automatically the size of the matrix needed to match with previous layer, 
    esp important for conv layer
    
    Replace Output layer with no weights, just output activation?
    Add batchnorm gamma and beta gradient checking
    Deconvolutional layer
    Recurrent layer
    LSTM layer
    GAN architecture
    Attention architecture
    L1 Regularization
