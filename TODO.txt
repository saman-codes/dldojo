BUGFIXES:

TO CHECK:

    can replace batch size parameter in layer functions with batch_size = self.error.shape[1]
    Check why net with batchnorm does worst than same net without
    -> working but understand implementations better
    Check Softmax derivative
    Check cross entropy

    Check minmax scaling implementation
    Check glorot normal and both he init implementations
    Check Dropout


TO ADD:
    Replace Output layer with no weights, just output activation?
    Convolutional layer

    Add batchnorm gamma and beta gradient checking
    Remove Save and load weights
    Deconvolutional layer
    Recurrent layer
    LSTM layer
    GAN architecture
    Attention architecture
    L1 Regularization
