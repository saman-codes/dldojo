TO CHECK:
    Check minmax scaling implementation
    Check glorot normal and both he init implementations
    Check Dropout


TO ADD:
    Convolutional layer
    Batch normalization
    Softmax implementation that doesn't depend on Sigmoid in last layer
    Deconvolutional layer
    Recurrent layer
    LSTM layer
    GAN architecture
    Attention architecture
    ADAM optimizer
    other optimizers ...
