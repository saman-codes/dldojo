# DL Dojo

My happy place to play around reimplementing the basic building blocks of deep learning with nothing fancier than Numpy. 

## Includes: 

[x] Gradient checking

[x] Batch Normalization

[x] Dropout

[x] L1 Regularization

[x] All main activations (ReLU, Leaky ReLU, Linear, Sigmoid, Softmax)

[x] A bunch of initializers (normal, uniform, ones, zers, Glorot Normal/Uniform, He Normal/Uniform)

[x] A bunch of optimizers (SGD, Momentum, Nesterov Momentum, Adagrad, RMSProp, Adam)
[x] MSE, Binary Cross Entropy and Categorical Cross Entropy losses

[x] MNIST prediction

[x] Weight plotting

## TODO and WIP:

[ ] Conv2D, DeConv and Pooling layers

[ ] RNN, LSTM and everything recurrent

[ ] Self-attention layer

*Is this the best possible way of implementing a neural network library?* Definitely not

*Could you think of a better way of doing this?* Yes, absolutely

*Why didn't you do X instead?* Either I didn't have the time, or I didn't think about it, or I didn't feel like it, or a mix of the three :) 

*Why didn't you implement a full autodiff library with computational graphs instead?* Because I started implementing stuff and kept going until I felt like it, and then I didn't feel like refactoring the whole project. But I'd love to that at some point in the future, if I ever have the time

*So, what's the point of this project?* Well, I learned a lot reimplementing all these things from scratch. Also, reading my implementations might help somebody else, I definitely learned a lot from reading other people's code. Also, maybe somebody will spot some errors, suggest a better implementation etc...

